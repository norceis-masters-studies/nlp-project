{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 346: expected 13 fields, saw 14\n",
      "Skipping line 367: expected 13 fields, saw 15\n",
      "Skipping line 466: expected 13 fields, saw 19\n",
      "Skipping line 467: expected 13 fields, saw 19\n",
      "Skipping line 494: expected 13 fields, saw 15\n",
      "Skipping line 615: expected 13 fields, saw 15\n",
      "Skipping line 616: expected 13 fields, saw 15\n",
      "Skipping line 617: expected 13 fields, saw 15\n",
      "Skipping line 618: expected 13 fields, saw 15\n",
      "Skipping line 619: expected 13 fields, saw 15\n",
      "Skipping line 620: expected 13 fields, saw 15\n",
      "Skipping line 712: expected 13 fields, saw 14\n",
      "Skipping line 713: expected 13 fields, saw 14\n",
      "Skipping line 739: expected 13 fields, saw 14\n",
      "Skipping line 747: expected 13 fields, saw 16\n",
      "Skipping line 867: expected 13 fields, saw 15\n",
      "Skipping line 956: expected 13 fields, saw 14\n",
      "Skipping line 1028: expected 13 fields, saw 14\n",
      "Skipping line 1234: expected 13 fields, saw 15\n",
      "Skipping line 1282: expected 13 fields, saw 17\n",
      "Skipping line 1326: expected 13 fields, saw 14\n",
      "Skipping line 1327: expected 13 fields, saw 14\n",
      "Skipping line 1337: expected 13 fields, saw 14\n",
      "Skipping line 1359: expected 13 fields, saw 14\n",
      "Skipping line 1401: expected 13 fields, saw 14\n",
      "Skipping line 1420: expected 13 fields, saw 14\n",
      "Skipping line 1496: expected 13 fields, saw 21\n",
      "Skipping line 1553: expected 13 fields, saw 16\n",
      "Skipping line 1615: expected 13 fields, saw 15\n",
      "Skipping line 1653: expected 13 fields, saw 14\n",
      "Skipping line 1684: expected 13 fields, saw 16\n",
      "Skipping line 1743: expected 13 fields, saw 16\n",
      "Skipping line 1774: expected 13 fields, saw 19\n",
      "Skipping line 1775: expected 13 fields, saw 16\n",
      "Skipping line 1792: expected 13 fields, saw 14\n",
      "Skipping line 1852: expected 13 fields, saw 14\n",
      "Skipping line 1875: expected 13 fields, saw 14\n",
      "Skipping line 1880: expected 13 fields, saw 14\n",
      "Skipping line 1955: expected 13 fields, saw 15\n",
      "Skipping line 1964: expected 13 fields, saw 16\n",
      "Skipping line 2077: expected 13 fields, saw 15\n",
      "Skipping line 2100: expected 13 fields, saw 16\n",
      "Skipping line 2113: expected 13 fields, saw 16\n",
      "Skipping line 2121: expected 13 fields, saw 14\n",
      "Skipping line 2182: expected 13 fields, saw 14\n",
      "Skipping line 2215: expected 13 fields, saw 16\n",
      "Skipping line 2255: expected 13 fields, saw 44\n",
      "Skipping line 2256: expected 13 fields, saw 44\n",
      "Skipping line 2330: expected 13 fields, saw 15\n",
      "Skipping line 2351: expected 13 fields, saw 14\n",
      "Skipping line 2372: expected 13 fields, saw 19\n",
      "Skipping line 2377: expected 13 fields, saw 44\n",
      "Skipping line 2381: expected 13 fields, saw 44\n",
      "Skipping line 2395: expected 13 fields, saw 44\n",
      "Skipping line 2397: expected 13 fields, saw 44\n",
      "Skipping line 2440: expected 13 fields, saw 14\n",
      "Skipping line 2443: expected 13 fields, saw 14\n",
      "Skipping line 2496: expected 13 fields, saw 44\n",
      "Skipping line 2497: expected 13 fields, saw 44\n",
      "Skipping line 2498: expected 13 fields, saw 16\n",
      "Skipping line 2584: expected 13 fields, saw 16\n",
      "Skipping line 2585: expected 13 fields, saw 44\n",
      "Skipping line 2641: expected 13 fields, saw 15\n",
      "Skipping line 2650: expected 13 fields, saw 15\n",
      "Skipping line 2841: expected 13 fields, saw 44\n",
      "Skipping line 2941: expected 13 fields, saw 14\n",
      "Skipping line 3071: expected 13 fields, saw 15\n",
      "Skipping line 3115: expected 13 fields, saw 44\n",
      "Skipping line 3174: expected 13 fields, saw 15\n",
      "Skipping line 3410: expected 13 fields, saw 44\n",
      "Skipping line 3487: expected 13 fields, saw 44\n",
      "Skipping line 3488: expected 13 fields, saw 16\n",
      "Skipping line 3529: expected 13 fields, saw 14\n",
      "Skipping line 3534: expected 13 fields, saw 14\n",
      "Skipping line 3624: expected 13 fields, saw 15\n",
      "Skipping line 3737: expected 13 fields, saw 44\n",
      "Skipping line 3738: expected 13 fields, saw 44\n",
      "Skipping line 3739: expected 13 fields, saw 44\n",
      "Skipping line 3781: expected 13 fields, saw 14\n",
      "Skipping line 3797: expected 13 fields, saw 15\n",
      "Skipping line 3852: expected 13 fields, saw 15\n",
      "Skipping line 3961: expected 13 fields, saw 14\n",
      "Skipping line 3971: expected 13 fields, saw 15\n",
      "Skipping line 4094: expected 13 fields, saw 14\n",
      "Skipping line 4120: expected 13 fields, saw 15\n",
      "Skipping line 4341: expected 13 fields, saw 14\n",
      "Skipping line 4370: expected 13 fields, saw 15\n",
      "Skipping line 4407: expected 13 fields, saw 44\n",
      "Skipping line 4434: expected 13 fields, saw 14\n",
      "Skipping line 4435: expected 13 fields, saw 14\n",
      "Skipping line 4520: expected 13 fields, saw 16\n",
      "Skipping line 4550: expected 13 fields, saw 15\n",
      "Skipping line 4656: expected 13 fields, saw 16\n",
      "Skipping line 4670: expected 13 fields, saw 14\n",
      "Skipping line 4705: expected 13 fields, saw 14\n",
      "Skipping line 4721: expected 13 fields, saw 16\n",
      "Skipping line 4723: expected 13 fields, saw 16\n",
      "Skipping line 4969: expected 13 fields, saw 14\n",
      "Skipping line 4986: expected 13 fields, saw 44\n",
      "Skipping line 5037: expected 13 fields, saw 15\n",
      "\n",
      "E:\\nlp2\\src\\util.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered.loc[:, 'Name'] = data_filtered.loc[:, 'Name'].str.lower()\n",
      "E:\\nlp2\\src\\util.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered.loc[:, 'Description'] = data_filtered.loc[:, 'Description'].str.lower()\n",
      "E:\\nlp2\\src\\util.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['First_date'] = data_filtered['Date'].min()\n",
      "E:\\nlp2\\src\\util.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['Date'] = pd.to_datetime(data_filtered.Date, format='%Y-%m-%d')\n",
      "E:\\nlp2\\src\\util.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['First_date'] = pd.to_datetime(data_filtered.First_date, format='%Y-%m-%d')\n",
      "E:\\nlp2\\src\\util.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['Days_passed'] = data_filtered['Date'] - data_filtered['First_date']\n",
      "E:\\nlp2\\src\\util.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['Days_passed_name'] = data_filtered['Days_passed'].astype(str) + ' ' + data_filtered['Name']\n",
      "E:\\nlp2\\src\\util.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['Days_passed_name_desc'] = data_filtered['Days_passed'].astype(str) + ' ' + \\\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import BertTokenizer, BertForMaskedLM, AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from util import preprocess_data\n",
    "\n",
    "SEED = 2137\n",
    "\n",
    "data = pd.read_csv('../data/recruitment_data_modified_python.csv',\n",
    "                   encoding='utf-8',\n",
    "                   sep=',',\n",
    "                   on_bad_lines='warn',\n",
    "                   quotechar='\"',\n",
    "                   doublequote=True,\n",
    "                   names=['URL', 'Voivodeship', 'Scrap_time', 'Name', 'Price', 'Brand', 'Condition', 'Offer_from', 'Type', 'Description', 'Added_at', 'Views', 'User_since'],\n",
    "                   skiprows=1)\n",
    "\n",
    "preprocessed_data = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "       Price                                   Days_passed_name  \\\n0     2799.0  52 days iphone 11 64 jak nowy 95% gwarancja wy...   \n1     2700.0  51 days iphone 11 64 gb czarny, idealny z gwar...   \n2     2899.0  51 days jak nowy apple iphone 11 256gbgb white...   \n3     2500.0  51 days apple iphone 11 biały 64gb - jak nowy ...   \n4     2150.0                51 days iphone 11 64 gb + gwarancja   \n...      ...                                                ...   \n2667  2299.0                       51 days iphone 11 black 64gb   \n2668  1900.0           51 days i phone 11 64 gb cena tylko dzis   \n2669  2800.0  51 days iphone 11  128 gb gwarancja , 100% bat...   \n2670  1650.0                       50 days iphone 11 white 64gb   \n2671  1700.0  51 days apple iphone 11 bez simlocka cały komp...   \n\n                                  Days_passed_name_desc  \n0     52 days iphone 11 64 jak nowy 95% gwarancja wy...  \n1     51 days iphone 11 64 gb czarny, idealny z gwar...  \n2     51 days jak nowy apple iphone 11 256gbgb white...  \n3     51 days apple iphone 11 biały 64gb - jak nowy ...  \n4     51 days iphone 11 64 gb + gwarancja witam, mam...  \n...                                                 ...  \n2667  51 days iphone 11 black 64gb sprzedam iphone 1...  \n2668  51 days i phone 11 64 gb cena tylko dzis cena ...  \n2669  51 days iphone 11  128 gb gwarancja , 100% bat...  \n2670  50 days iphone 11 white 64gb na sprzedaż posia...  \n2671  51 days apple iphone 11 bez simlocka cały komp...  \n\n[2672 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Price</th>\n      <th>Days_passed_name</th>\n      <th>Days_passed_name_desc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2799.0</td>\n      <td>52 days iphone 11 64 jak nowy 95% gwarancja wy...</td>\n      <td>52 days iphone 11 64 jak nowy 95% gwarancja wy...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2700.0</td>\n      <td>51 days iphone 11 64 gb czarny, idealny z gwar...</td>\n      <td>51 days iphone 11 64 gb czarny, idealny z gwar...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2899.0</td>\n      <td>51 days jak nowy apple iphone 11 256gbgb white...</td>\n      <td>51 days jak nowy apple iphone 11 256gbgb white...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2500.0</td>\n      <td>51 days apple iphone 11 biały 64gb - jak nowy ...</td>\n      <td>51 days apple iphone 11 biały 64gb - jak nowy ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2150.0</td>\n      <td>51 days iphone 11 64 gb + gwarancja</td>\n      <td>51 days iphone 11 64 gb + gwarancja witam, mam...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2667</th>\n      <td>2299.0</td>\n      <td>51 days iphone 11 black 64gb</td>\n      <td>51 days iphone 11 black 64gb sprzedam iphone 1...</td>\n    </tr>\n    <tr>\n      <th>2668</th>\n      <td>1900.0</td>\n      <td>51 days i phone 11 64 gb cena tylko dzis</td>\n      <td>51 days i phone 11 64 gb cena tylko dzis cena ...</td>\n    </tr>\n    <tr>\n      <th>2669</th>\n      <td>2800.0</td>\n      <td>51 days iphone 11  128 gb gwarancja , 100% bat...</td>\n      <td>51 days iphone 11  128 gb gwarancja , 100% bat...</td>\n    </tr>\n    <tr>\n      <th>2670</th>\n      <td>1650.0</td>\n      <td>50 days iphone 11 white 64gb</td>\n      <td>50 days iphone 11 white 64gb na sprzedaż posia...</td>\n    </tr>\n    <tr>\n      <th>2671</th>\n      <td>1700.0</td>\n      <td>51 days apple iphone 11 bez simlocka cały komp...</td>\n      <td>51 days apple iphone 11 bez simlocka cały komp...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2672 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "0    52 days iphone 11 64 jak nowy 95% gwarancja wy...\n1    51 days iphone 11 64 gb czarny, idealny z gwar...\n2    51 days jak nowy apple iphone 11 256gbgb white...\n3    51 days apple iphone 11 biały 64gb - jak nowy ...\n4                  51 days iphone 11 64 gb + gwarancja\n5                  51 days iphone 11 64 gb + gwarancja\n6             51 days iphone 11 w bardzo dobrym stanie\n7    51 days iphone 11 * idealny stan * 100% bateri...\n8    51 days iphone 11 128 gb stan idealny etui gratis\n9     52 days iphone 11 64 gb prawie nowy super zestaw\nName: Days_passed_name, dtype: object"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data['Days_passed_name'][:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained('dkleczek/bert-base-polish-uncased-v1')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# encoded_corpus = tokenizer(text=preprocessed_data['Days_passed_name'].tolist(),\n",
    "#                             add_special_tokens=True,\n",
    "#                             padding='max_length',\n",
    "#                             truncation='longest_first',\n",
    "#                             max_length=50,\n",
    "#                             return_attention_mask=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# input_ids = np.array(encoded_corpus['input_ids'])\n",
    "# attention_mask = np.array(encoded_corpus['attention_mask'])\n",
    "# labels = np.asarray(preprocessed_data['Price'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# train_inputs, test_inputs, train_labels, test_labels = train_test_split(input_ids, labels, train_size=0.8, random_state=SEED)\n",
    "# train_inputs, val_inputs, train_labels, val_labels = train_test_split(train_inputs, train_labels, train_size=0.8, random_state=SEED)\n",
    "# train_masks, test_masks, train_masks_labels, _ = train_test_split(attention_mask, labels, train_size=0.8, random_state=SEED)\n",
    "# train_masks, val_masks, _, _ = train_test_split(train_masks, train_masks_labels, train_size=0.8, random_state=SEED)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# scaler.fit(train_labels.reshape(-1, 1))\n",
    "#\n",
    "# train_labels = scaler.transform(train_labels.reshape(-1,1))\n",
    "# test_labels = scaler.transform(test_labels.reshape(-1,1))\n",
    "# val_labels = scaler.transform(val_labels.reshape(-1,1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "# def create_dataloaders(inputs, masks, labels, batch_size):\n",
    "#     input_tensor = torch.tensor(inputs)\n",
    "#     mask_tensor = torch.tensor(masks)\n",
    "#     labels_tensor = torch.tensor(labels)\n",
    "#     dataset = TensorDataset(input_tensor, mask_tensor, labels_tensor)\n",
    "#     dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "#     return dataloader\n",
    "#\n",
    "# train_dataloader = create_dataloaders(train_inputs, train_masks, train_labels, batch_size)\n",
    "# test_dataloader = create_dataloaders(test_inputs, test_masks, test_labels, batch_size)\n",
    "# val_dataloader = create_dataloaders(val_inputs, val_masks, val_labels, batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dkleczek/bert-base-polish-uncased-v1 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# class BertRegressor(nn.Module):\n",
    "#\n",
    "#     def __init__(self, drop_rate=0.2):\n",
    "#\n",
    "#         super(BertRegressor, self).__init__()\n",
    "#         D_in, D_out = 768, 1\n",
    "#\n",
    "#         self.bert = AutoModelForSequenceClassification.from_pretrained('dkleczek/bert-base-polish-uncased-v1', num_labels=1)\n",
    "#         self.regressor = nn.Sequential(\n",
    "#             nn.Dropout(drop_rate),\n",
    "#             nn.Linear(self.bert.config.hidden_size, D_out))\n",
    "#\n",
    "#     def forward(self, input_ids, attention_masks):\n",
    "#\n",
    "#         outputs = self.bert(input_ids, attention_masks, return_dict=False)\n",
    "#         outputs = self.regressor(outputs[0])\n",
    "#\n",
    "#         return outputs\n",
    "#\n",
    "# model = BertRegressor()\n",
    "# optimizer = torch.optim.AdamW(model.parameters(),\n",
    "#                   lr=5e-5,\n",
    "#                   eps=1e-8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# from transformers import get_linear_schedule_with_warmup\n",
    "#\n",
    "# EPOCHS = 5\n",
    "# total_steps = len(train_dataloader) * EPOCHS\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "# loss_function = nn.MSELoss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-----\n",
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1600x60000 and 768x1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 25\u001B[0m\n\u001B[0;32m     21\u001B[0m             scheduler\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n\u001B[1;32m---> 25\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_function\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mEPOCHS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[43m              \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcpu\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclip_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[29], line 15\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, optimizer, scheduler, loss_function, epochs, train_dataloader, device, clip_value)\u001B[0m\n\u001B[0;32m     13\u001B[0m batch_inputs, batch_masks, batch_labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(b\u001B[38;5;241m.\u001B[39mto(device) \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m batch)\n\u001B[0;32m     14\u001B[0m model\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 15\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_masks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_function(outputs\u001B[38;5;241m.\u001B[39msqueeze(),\n\u001B[0;32m     17\u001B[0m                  batch_labels\u001B[38;5;241m.\u001B[39msqueeze())\n\u001B[0;32m     18\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32mE:\\nlp2\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[28], line 17\u001B[0m, in \u001B[0;36mBertRegressor.forward\u001B[1;34m(self, input_ids, attention_masks)\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, input_ids, attention_masks):\n\u001B[0;32m     16\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbert(input_ids, attention_masks, return_dict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m---> 17\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mregressor\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[1;32mE:\\nlp2\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mE:\\nlp2\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    203\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 204\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    205\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32mE:\\nlp2\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mE:\\nlp2\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (1600x60000 and 768x1)"
     ]
    }
   ],
   "source": [
    "# from torch.nn.utils.clip_grad import clip_grad_norm\n",
    "#\n",
    "# def train(model, optimizer, scheduler, loss_function, epochs,\n",
    "#           train_dataloader, device, clip_value=2):\n",
    "#\n",
    "#     for epoch in range(epochs):\n",
    "#         print(epoch)\n",
    "#         print(\"-----\")\n",
    "#         best_loss = 1e10\n",
    "#         model.train()\n",
    "#         for step, batch in enumerate(train_dataloader):\n",
    "#             print(step)\n",
    "#             batch_inputs, batch_masks, batch_labels = tuple(b.to(device) for b in batch)\n",
    "#             model.zero_grad()\n",
    "#             outputs = model(batch_inputs, batch_masks)\n",
    "#             loss = loss_function(outputs.squeeze(),\n",
    "#                              batch_labels.squeeze())\n",
    "#             loss.backward()\n",
    "#             clip_grad_norm(model.parameters(), clip_value)\n",
    "#             optimizer.step()\n",
    "#             scheduler.step()\n",
    "#\n",
    "#     return model\n",
    "#\n",
    "# model = train(model, optimizer, scheduler, loss_function, EPOCHS,\n",
    "#               train_dataloader, torch.device(\"cpu\"), clip_value=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dkleczek/bert-base-polish-uncased-v1 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dkleczek/bert-base-polish-uncased-v1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_squared_error, mean_absolute_error\n",
    "model = AutoModelForSequenceClassification.from_pretrained('dkleczek/bert-base-polish-uncased-v1', num_labels = 1).to(\"cpu\")\n",
    "\n",
    "def compute_metrics_for_regression(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    labels = labels.reshape(-1, 1)\n",
    "\n",
    "    mse = mean_squared_error(labels, logits)\n",
    "    rmse = mean_squared_error(labels, logits, squared=False)\n",
    "    mae = mean_absolute_error(labels, logits)\n",
    "    r2 = r2_score(labels, logits)\n",
    "    smape = 1/len(labels) * np.sum(2 * np.abs(logits-labels) / (np.abs(labels) + np.abs(logits))*100)\n",
    "\n",
    "    return {\"mse\": mse, \"rmse\": rmse, \"mae\": mae, \"r2\": r2, \"smape\": smape}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\cubix/.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-uncased-v1\\snapshots\\62be9821055981deafb23f217b68cc41f38cdb76\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-uncased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\",\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\cubix/.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-uncased-v1\\snapshots\\62be9821055981deafb23f217b68cc41f38cdb76\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\cubix/.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-uncased-v1\\snapshots\\62be9821055981deafb23f217b68cc41f38cdb76\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\cubix/.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-uncased-v1\\snapshots\\62be9821055981deafb23f217b68cc41f38cdb76\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\cubix/.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-uncased-v1\\snapshots\\62be9821055981deafb23f217b68cc41f38cdb76\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-uncased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\",\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\cubix/.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-uncased-v1\\snapshots\\62be9821055981deafb23f217b68cc41f38cdb76\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-uncased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\",\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "E:\\nlp2\\venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2137\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 170\n",
      "  Number of trainable parameters = 132122113\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 61\u001B[0m\n\u001B[0;32m     52\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m     53\u001B[0m     model \u001B[38;5;241m=\u001B[39m model,\n\u001B[0;32m     54\u001B[0m     args \u001B[38;5;241m=\u001B[39m training_args,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     57\u001B[0m     compute_metrics \u001B[38;5;241m=\u001B[39m compute_metrics_for_regression,\n\u001B[0;32m     58\u001B[0m )\n\u001B[0;32m     60\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m---> 61\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;66;03m# Call the summary\u001B[39;00m\n\u001B[0;32m     64\u001B[0m trainer\u001B[38;5;241m.\u001B[39mevaluate()\n",
      "File \u001B[1;32mE:\\nlp2\\venv\\lib\\site-packages\\transformers\\trainer.py:1543\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1538\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_wrapped \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\n\u001B[0;32m   1540\u001B[0m inner_training_loop \u001B[38;5;241m=\u001B[39m find_executable_batch_size(\n\u001B[0;32m   1541\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inner_training_loop, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_batch_size, args\u001B[38;5;241m.\u001B[39mauto_find_batch_size\n\u001B[0;32m   1542\u001B[0m )\n\u001B[1;32m-> 1543\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1544\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1545\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1546\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1547\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1548\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\nlp2\\venv\\lib\\site-packages\\transformers\\trainer.py:1765\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   1762\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_load_rng_state(resume_from_checkpoint)\n\u001B[0;32m   1764\u001B[0m step \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1765\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, inputs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(epoch_iterator):\n\u001B[0;32m   1766\u001B[0m \n\u001B[0;32m   1767\u001B[0m     \u001B[38;5;66;03m# Skip past any already trained steps if resuming training\u001B[39;00m\n\u001B[0;32m   1768\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m steps_trained_in_current_epoch \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1769\u001B[0m         steps_trained_in_current_epoch \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32mE:\\nlp2\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mE:\\nlp2\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mE:\\nlp2\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mE:\\nlp2\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[1;32mIn[16], line 23\u001B[0m, in \u001B[0;36mMakeTorchData.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[1;32m---> 23\u001B[0m     item \u001B[38;5;241m=\u001B[39m {k: torch\u001B[38;5;241m.\u001B[39mtensor(v[idx]) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencodings\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m     24\u001B[0m     item[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabels[idx]])\n\u001B[0;32m     25\u001B[0m     item[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(item[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "Cell \u001B[1;32mIn[16], line 23\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[1;32m---> 23\u001B[0m     item \u001B[38;5;241m=\u001B[39m {k: torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[43mv\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencodings\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m     24\u001B[0m     item[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabels[idx]])\n\u001B[0;32m     25\u001B[0m     item[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(item[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "\u001B[1;31mIndexError\u001B[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('dkleczek/bert-base-polish-uncased-v1')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data['Days_passed_name'], preprocessed_data['Price'], train_size=0.8)\n",
    "\n",
    "\n",
    "# encoded_corpus = tokenizer(text=preprocessed_data['Days_passed_name'].tolist(),\n",
    "#                             add_special_tokens=True,\n",
    "#                             padding='max_length',\n",
    "#                             truncation='longest_first',\n",
    "#                             max_length=50,\n",
    "#                             return_attention_mask=True)\n",
    "\n",
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=50)\n",
    "valid_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=50)\n",
    "\n",
    "\n",
    "class MakeTorchData(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        item[\"labels\"] = float(item[\"labels\"])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# convert our tokenized data into a torch Dataset\n",
    "train_dataset = MakeTorchData(X_train, y_train.ravel())\n",
    "valid_dataset = MakeTorchData(X_test, y_test.ravel())\n",
    "\n",
    "# Specifiy the arguments for the trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir ='../results',\n",
    "    num_train_epochs = 5,\n",
    "    per_device_train_batch_size = 64,\n",
    "    per_device_eval_batch_size = 20,\n",
    "    weight_decay = 0.01,\n",
    "    learning_rate = 2e-5,\n",
    "    logging_dir = './logs',\n",
    "    save_total_limit = 10,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'rmse',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    ")\n",
    "\n",
    "# Call the Trainer\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = valid_dataset,\n",
    "    compute_metrics = compute_metrics_for_regression,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Call the summary\n",
    "trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
